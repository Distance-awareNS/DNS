{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary packages for training\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_cluster import random_walk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pylab as py\n",
    "from sklearn.metrics import f1_score\n",
    "# from sklearn.multiclass import OneVsRestClassifier #import this for multiclass classification (like PPI dataset)\n",
    "import scipy.sparse as sp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.plot_settings import global_settings \n",
    "global_settings() # dont call this if you don't have latex installed in your device\n",
    "from scripts.load import load_data, load_shrtst_dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs of the model, change the paths and hyperparameter accordingly\n",
    "\n",
    "# data folder\n",
    "data_folder = './data/CiteSeer'\n",
    "# folder to save the models\n",
    "model_folder = 'saved_models/CiteSeer/UNS'\n",
    "# parameter for the DNS model\n",
    "embedding_dim = 128\n",
    "walk_length = 20 # random walk length; this is different from the context window\n",
    "context_size = 3\n",
    "walks_per_node = 50\n",
    "num_negative_samples = 20\n",
    "epochs = 31\n",
    "runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded!\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data, edgelist, y, train_mask, test_mask, val_mask = load_data(data_folder)\n",
    "shrtst_dist, G = load_shrtst_dist_matrix(edgelist=edgelist, is_available=True, folder_name=data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Specify the cuda; if not available it will use cpu\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "data = data.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-15\n",
    "\n",
    "class DeepWalk_Unigram(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, walk_length, context_size,\n",
    "                 walks_per_node = 1, p = 1, q = 1, num_negative_samples=None):\n",
    "        super(DeepWalk_Unigram, self).__init__()\n",
    "        assert walk_length >= context_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.walk_length = walk_length - 1\n",
    "        self.context_size = context_size\n",
    "        self.walks_per_node = walks_per_node\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\" Resets the embeddings \"\"\"\n",
    "        self.embedding.reset_parameters()\n",
    "        \n",
    "    def forward(self, subset):\n",
    "        \"\"\" Returns the embeddings for the nodes in subset\"\"\"\n",
    "        return self.embedding(subset)\n",
    "    \n",
    "    def __random_walk__(self, edge_index, subset = None):\n",
    "        \n",
    "        if subset is None:\n",
    "            subset = torch.arange(self.num_nodes, device = edge_index.device)\n",
    "        subset = subset.repeat(self.walks_per_node)\n",
    "        \n",
    "        rw = random_walk(edge_index[0], edge_index[1], subset,\n",
    "                        self.walk_length, self.p, self.q, self.num_nodes)\n",
    "        \n",
    "        walks = []\n",
    "        num_walks_per_rw = 1 + self.walk_length + 1 - self.context_size\n",
    "        \n",
    "        for j in range(num_walks_per_rw):\n",
    "            walks.append(rw[:, j:j + self.context_size])\n",
    "        return torch.cat(walks, dim=0)\n",
    "    \n",
    "    def loss(self, edge_index, subset=None):\n",
    "        \n",
    "        walk = self.__random_walk__(edge_index, subset)\n",
    "        start, rest = walk[:, 0], walk[:, 1:].contiguous()\n",
    "        \n",
    "        h_start = self.embedding(start).view(\n",
    "                walk.size(0), 1, self.embedding_dim)\n",
    "        \n",
    "        h_rest = self.embedding(rest.view(-1)).view(\n",
    "                walk.size(0), rest.size(1), self.embedding_dim)\n",
    "        \n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(out) + EPS).mean()\n",
    "        \n",
    "        # Negative sampling loss.\n",
    "        num_negative_samples = self.num_negative_samples\n",
    "        if num_negative_samples is None:\n",
    "            num_negative_samples = rest.size(1)\n",
    "        \n",
    "        neg_sample = torch.randint(self.num_nodes,\n",
    "                                  (walk.size(0), num_negative_samples),\n",
    "                                  dtype=torch.long, device=edge_index.device)\n",
    "        \n",
    "        h_neg_rest = self.embedding(neg_sample)\n",
    "        out = (h_start * h_neg_rest).sum(dim=-1).view(-1)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid(out) + EPS).mean()\n",
    "        \n",
    "        return pos_loss + neg_loss\n",
    "    \n",
    "    def test(self, train_z, train_y, test_z, test_y, solver='lbfgs',\n",
    "             multi_class='auto', *args, **kwargs):\n",
    "        r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
    "        task.\"\"\"\n",
    "        clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
    "                                 **kwargs).fit(train_z.detach().cpu().numpy(),\n",
    "                                               train_y.detach().cpu().numpy())\n",
    "        pred_label, true_label = clf.predict(test_z.detach().cpu().numpy()), test_y.detach().cpu().numpy()\n",
    "        return f1_score(true_label, pred_label, average='macro')\n",
    "    \n",
    "    def test_predict(self, train_z, train_y, test_z, test_y, solver='lbfgs',\n",
    "             multi_class='auto', *args, **kwargs):\n",
    "        r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
    "        task.\"\"\"\n",
    "        clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
    "                                 **kwargs).fit(train_z.detach().cpu().numpy(),\n",
    "                                               train_y.detach().cpu().numpy())\n",
    "        return clf.predict(test_z.detach().cpu().numpy()), test_y.detach().cpu().numpy()\n",
    "    \n",
    "    def test_predict_f1(self, train_z, train_y, test_z, test_y, solver='lbfgs',\n",
    "             multi_class='auto', *args, **kwargs):\n",
    "        r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
    "        task.\"\"\"\n",
    "        pred_label, true_label = self.test_predict(train_z, train_y, test_z, test_y)\n",
    "        f1_macro = f1_score(true_label, pred_label, average='macro')\n",
    "        f1_micro = f1_score(true_label, pred_label, average='micro')\n",
    "        return f1_macro, f1_micro\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, p={}, q={})'.format(\n",
    "            self.__class__.__name__, self.num_nodes, self.embedding_dim,\n",
    "            self.p, self.q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for subset in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(data.edge_index, subset.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For context window 3  :\n",
      "run  0: ..............................0.47694451790979553\n",
      "run  1: ..............................0.4870179089977669\n",
      "run  2: ..............................0.48019566345013615\n",
      "run  3: ..............................0.465347836426757\n",
      "run  4: ..............................0.4685312749194621\n"
     ]
    }
   ],
   "source": [
    "# we train the UNS model using the hyperparamets set by the user\n",
    "# we used LR as the downstream model\n",
    "# to show the results : we choose the best model from the 30 epochs\n",
    "# based on its downstream performance on validation set\n",
    "# we save the model in saved_models\n",
    "\n",
    "\n",
    "print('For context window',context_size,' :')\n",
    "for run in range(runs):\n",
    "    loader = DataLoader(torch.arange(data.num_nodes), batch_size=8, shuffle=True)\n",
    "    model = DeepWalk_Unigram(data.num_nodes, embedding_dim = embedding_dim, walk_length=walk_length, \\\n",
    "                         context_size=context_size, walks_per_node=walks_per_node, num_negative_samples=num_negative_samples)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    best_acc = -1\n",
    "    print('run ', run, end=': ')\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        loss = train()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            z = model(torch.arange(data.num_nodes, device=device))\n",
    "\n",
    "        # comment the following snipet for multiclass classification\n",
    "        \n",
    "        test_f1_mac, _ = model.test_predict_f1(z[data.train_mask], data.y[data.train_mask],\n",
    "                                 z[data.test_mask], data.y[data.test_mask], max_iter=150)\n",
    "\n",
    "        if test_f1_mac > best_acc:\n",
    "            best_acc = test_f1_mac\n",
    "#             model_name = model_folder+'/cw_'+str(contex_size)+'_run_'+str(run)+'.pt'\n",
    "#             torch.save(model, model_name)\n",
    "\n",
    "        print('.', end='')\n",
    "    \n",
    "#             for multiclass classification : uncomment the following snipet\n",
    "\n",
    "#             z_train = z.data.cpu().numpy()[train_mask]\n",
    "#             z_test = z.data.cpu().numpy()[test_mask]\n",
    "#             y_train = data.y.data.cpu().numpy()[train_mask]\n",
    "#             y_test = data.y.data.cpu().numpy()[test_mask]\n",
    "#             clf = OneVsRestClassifier(LogisticRegression(solver='lbfgs', multi_class='auto')).fit(z_train, y_train)\n",
    "#             y_p = clf.predict(z_test).ravel()\n",
    "#             y_t = y_test.ravel()\n",
    "\n",
    "#             test_f1_mac = f1_score(y_t, y_p, average='macro')\n",
    "#             if test_f1_mac > best_acc:\n",
    "#                 best_acc = test_f1_mac\n",
    "#                 model_name = out_folder+'/cw_'+str(ci)+'_run_'+str(run)+'.pt'\n",
    "#                 torch.save(model, model_name)\n",
    "\n",
    "#             print('.', end='')\n",
    "            \n",
    "    print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
